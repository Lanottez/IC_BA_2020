---
title: "ISLR Chapter 5 Lab Notebook"
output: html_notebook
---
This R Notebook has been constructed from the Chapter 5 Lab of ISLR by James, Witten, Hastie and Tibshirani. It has been edited to reflect the content of Advanced ML at ICBS and covers Cross-Validation and the Bootstrap


# The Validation Set Approach
```{r}
library(ISLR)

set.seed(1)
train = sample(392,196)   # A random vector of length 196 with elements in 1:392  
# There are 392 observations in the Auto dataset
lm.fit = lm(mpg~horsepower,data=Auto,subset=train)
attach(Auto) 
mean((mpg-predict(lm.fit,Auto))[-train]^2)    # Only applying to observations not in the training set
```

### Quadratic Regression
```{r}
lm.fit2 = lm(mpg~poly(horsepower,2),data=Auto,subset=train)   
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
```

### Cubic Regression
```{r}
lm.fit3 = lm(mpg~poly(horsepower,3),data=Auto,subset=train)  
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
```

### Choose a Different Seed so that we can rerun previous commands with a different training set
```{r}
set.seed(2)  
train=sample(392,196)
lm.fit = lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2  =lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3 = lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
```

# Leave-One-Out Cross-Validation
```{r}
glm.fit = glm(mpg~horsepower,data=Auto)   
# glm performs linear regression by default if a "family" is not specified
coef(glm.fit)
```

```{r}
lm.fit = lm(mpg~horsepower,data=Auto)
coef(lm.fit)  
```
The output from glm and lm are therefore the same but we prefer glm here because we can use cv.glm() which is part of the boot library. (Of course glm can also fit other models that lm cannot fit.)

```{r}
library(boot)
glm.fit = glm(mpg~horsepower,data=Auto)
cv.err = cv.glm(Auto,glm.fit)
cv.err$delta   
```
The 2 numbers in delta are the LOOCV error estimate (identical here up to 2 decimal places). (The 2nd one is actually a biased corrected estimate of the LOOCV error.)

Now will do the same for polynomial regression of order i for i = 1,...,5
```{r}
cv.error = rep(0,5)  
for (i in 1:5){
  glm.fit = glm(mpg~poly(horsepower,i),data=Auto)
  cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error  
```

Again we see a big improvement going from linear to quadratic but no real improvement beyond that

# k-Fold Cross-Validation
```{r}
set.seed(17)
cv.error.10 = rep(0,10)
for (i in 1:10){
  glm.fit = glm(mpg~poly(horsepower,i),data=Auto)
  cv.error.10[i] = cv.glm(Auto,glm.fit,K=10)$delta[1]   # Doing K=10-fold cross-validation
}
cv.error.10 
```
And again we see a big improvement going from linear to quadratic but no real improvement beyond that


# Bootstrap
We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear
regression model on the Auto data set.

### Estimating the Accuracy of a Statistic of Interest
One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations
are required. Performing a bootstrap analysis in R entails only two steps. First, we must create a function that computes the statistic of interest.
Second, we use the boot() function, which is part of the boot library, to **boot()** perform the bootstrap by repeatedly sampling observations from the data
set with replacement.


The Portfolio data set in the ISLR package is described in Section 5.2. To illustrate the use of the bootstrap on this data, we must first create
a function, alpha.fn(), which takes as input the $(X, Y)$ data as well as a vector indicating which observations should be used to estimate $\alpha$. The
function then outputs the estimate for $\alpha$ based on the selected observations.

```{r}
# First must create a function to compute the statistic of interest
alpha.fn = function(data,index){     
  # Input args are: (i) the data and (ii) an index vector defining what observations to use
  X = data$X[index]
  Y = data$Y[index]
  return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
```

Portfolio is a dataset of 100 joint returns of assets X and Y
```{r}
head(Portfolio)
```
The function alpha.fn returns, or outputs, an estimate for ?? based on applying (5.7) (in ISLR) to the observations indexed by the argument index. For instance, the
following command tells R to estimate $\alpha$ using all 100 observations.

```{r}
alpha.fn(Portfolio,1:100)
```
The next command uses the **sample()** function to randomly select 100 observations from the range 1 to 100, with replacement. This is equivalent
to constructing a new bootstrap data set and recomputing $\hat{\alpha}$ based on the new data set.
```{r}
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))   
```

Now let's use the **boot** function to perform the bootstrap for us. Below we produce $R = 1,000$ bootstrap estimates for $\alpha$.
```{r}
boot(Portfolio,alpha.fn,R=1000)  
```

### Estimating the Accuracy of a Linear Regression Model
The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here
we use the bootstrap approach in order to assess the variability of the estimates for $\beta_0$ and $\beta_1$, the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained
using the formulas for $\mbox{SE}(\hat{\beta}_0)$ and $\mbox{SE}(\hat{\beta}_1)$ described in Section 3.1.2 of ISLR.

We first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept
and slope estimates for the linear regression model. We then apply this function to the full set of 392 observations in order to compute the estimates
of $\beta_0$ and $\beta_1$ on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3. (Note that we do not need the
{ and } at the beginning and end of the function because it is only one line long.)
```{r}
boot.fn = function(data,index)
  return(coef(lm(mpg~horsepower,data=data,subset=index)))  
# Don't need {} to define the function since it's only 1 line long
boot.fn(Auto,1:392)
```
... so function works correctly.

The boot.fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among
the observations with replacement. Here we give two examples.
```{r}
set.seed(1)
boot.fn(Auto,sample(392,392,replace=T))   # a single bootstrap
boot.fn(Auto,sample(392,392,replace=T))   # and another
```
Previous 2 calls to boot.fn generated 2 bootstrap samples.

Next, we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.
```{r}
boot(Auto,boot.fn,1000)  # Using the boot function to run a bootstrap with 1000 samples
```
This indicates that the bootstrap estimate for $\mbox{SE}(\hat{\beta}_0)$ is 0.86, and that the bootstrap estimate for $\mbox{SE}(\hat{\beta}_1)$ is 0.0074. As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.

```{r}
summary(lm(mpg~horsepower,data=Auto))$coef 
```
The standard error estimates for $\hat{\beta}_0$ and $\hat{\beta}_1$ obtained using the formulas from Section 3.1.2 are 0.717 for the intercept and 0.0064 for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In
fact, it suggests the opposite. Recall that the standard formulas given in Equation 3.8 on page 66 rely on certain assumptions. For example, they
depend on the unknown parameter ??2, the noise variance.We then estimate $\sigma^2$ using the RSS. Now although the formula for the standard errors do not
rely on the linear model being correct, the estimate for $\sigma^2$ does. We see in Figure 3.8 on page 91 that there is a non-linear relationship in the data, and
so the residuals from a linear fit will be inflated, and so will $\hat{\sigma}^2$. Secondly, the standard formulas assume (somewhat unrealistically) that the $x_i$ are fixed, and all the variability comes from the variation in the errors $\epsilon_i$. The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of $\hat{\beta}_0$ and $\hat{\beta}_1$ than is the summary() function.


Below we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and
the standard estimates of $\mbox{SE}(\hat{\beta}_0)$, $\mbox{SE}(\hat{\beta}_1)$ and $\mbox{SE}(\hat{\beta}_2)$.
```{r}
boot.fn = function(data,index)   
  + coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))
set.seed(1)
boot(Auto,boot.fn,1000)
```

And now compare with the output from the corresponding lm() regression....
```{r}
summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef   
```
Note much closer agreement now between bootstrap and lm() estimates of standard errors! Why?

