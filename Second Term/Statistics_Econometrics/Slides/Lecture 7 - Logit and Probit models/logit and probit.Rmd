---
title: "Logit and Probit Models"
author: Jiahua Wu
subtitle: Statistics and Econometrics
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(car)
library(dplyr)
library(stargazer)
library(lmtest)
library(caret)
```

## Example 17.1 (slides 17-18)
The function to estimate logit and profit models is $glm()$. When we estimate logit (probit), we need specify $family = ``binomial"$ (as the response follows a Bernoulli distribution, which is a special case of binomial distribution), and $link=``logit"$ for logit model and $link=``probit"$ for probit model.

```{r, results='asis'}
# Example 17.1
load("mroz.RData")

# estimation of binary response models
inlf.lpm <- glm(inlf ~ nwifeinc + educ + exper + expersq + age 
                + kidslt6 + kidsge6, family = "gaussian", data)

inlf.lpm.lm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age 
                  + kidslt6 + kidsge6, data)

inlf.probit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age 
                   + kidslt6 + kidsge6, family = "binomial"(link = "probit"), data)

inlf.logit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age
                  + kidslt6 + kidsge6, family = "binomial"(link = "logit"), data)
stargazer(inlf.lpm.lm, inlf.lpm, inlf.probit, inlf.logit, header = FALSE, type = 'latex', 
          title = "Example 17.1. Labour Force Participation")
```

```{r}
summary(inlf.logit)
```
First, let us try to understand the output of $glm()$. We take the logit model as an example. The first thing we observe from the summary is deviance residuals. There are five different types of residuals from $glm$ models (see details: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.summaries.html). The most commonly used one, and the one reported in summary by default is deviance residuals. The sum of squared of deviance residuals equals to the deviance of the estimated model. We can verify this by coding it manually as follows.
```{r}
# deviance = sum of squared deviance residuals
summary(residuals(inlf.logit, type = "deviance"))
sum(residuals(inlf.logit, type = "deviance")^2)
inlf.logit$deviance

# deviance = -2 * log likelihood
(-2) * logLik(inlf.logit)
```
The deviance of the model itself equals to -2 times the log-likelihood of the model. We can think of deviance as the couterpart of sum of squared residuals - we want to minimize sum of squared residuals in regression models, whereas deivance is minimized for logit and profit models (equivalent to maximizing log likelihood). The deviance residual is one way to measure the fit of each observation, just like residuals in multiple regression.

The interpretation of coefficients, standard errors and etc. is similar to that in linear regression models. One slight difference is that, instead of calling it $t$ test, we refer to it as $z$ test, as now with logit and probit models, the test statistic follows a standard normal distribution.

Dispersion parameter is not really useful for logit/probit models. It is mainly used in Poisson models to allow for different mean and variance (theoretically, mean and variance is always the same for Poisson distributed random variables). In practice, for any count data, we can well observe that mean and variance is different, and this dispersion parameter allows for a more flexbile model to fit the data.

Lastly, the maximum likelihood estimates are estimated by solving an optimization problem. Under the hood, it relies on an iterative algorithm (Newton-Raphson algorithm) to solve the problem. "Number of Fisher Scoring iterations" indicates the number of iterations required before convergence.

In the class, we discussed several different measures of goodness of fit. The following code shows how to calculate each of them. 
```{r}
### goodness of fit
# pseudo r-squared
1 - inlf.logit$deviance/inlf.logit$null.deviance

# information criteria
AIC(inlf.logit)
BIC(inlf.logit)
```

The confusion matrix is a common tool for evaluating prediction performance of logit and profit models. We can use the function "confusionMatrix" from $caret$ package to create confusion matrix. The interpretation of confusion matrix is discussed in the slides 13-14. Based on confusion matrix, we can calculate various metrics that can be used to compare different models. Three measures we discussed in the class are "Accuracy", "Sensitivity" (i.e., recall) and "Pos Pred Value" (i.e., precision). 
```{r}
inlf.predicted <- ifelse(inlf.logit$fitted.values < 0.5, 0, 1)
inlf.predicted <- as.factor(inlf.predicted)
confusionMatrix(inlf.predicted, as.factor(data$inlf), positive = "1")
```

When it comes to hypothesis tesing, there are two functions that we can use, one is $lrtest()$ from the $lmtest$ package, and the other one is $linearHypothesis()$ from the $car$ package. Both of them are based on the likelihood ratio test. We reject null when test statistic is sufficiently large ($p$ value is sufficiently small).
```{r}
# test for overall significance
lrtest(inlf.logit)

# test of overall significance manually
1 - pchisq(inlf.logit$null.deviance - inlf.logit$deviance, 
           df = inlf.logit$df.null - inlf.logit$df.residual)

# hypothesis testing
linearHypothesis(inlf.logit, c("exper = 0", "expersq = 0"))
```

Similar to multiple regression models, we can use $confint()$ for confidence intervals. By default, the function produces 95% confidence intervals for all model coefficients.
```{r}
# confidence intervals
confint(inlf.logit)
```

$step()$ function still works for probit and logit, and the interpretation of output is exactly the same as that in linear regressions.
```{r, eval=FALSE}
### model selection
inlf.null <- glm(inlf ~ 1, family = "binomial"(link = "logit"), data)
inlf.full <- glm(inlf ~ nwifeinc + educ + exper + expersq + age
                + kidslt6 + kidsge6, family = "binomial"(link = "logit"), data)
step(inlf.null, scope = list(lower = inlf.null, upper = inlf.full), direction = "forward")
step(inlf.full, direction = "backward")
```

$predict()$ function comes with two types, link or response. When $type = ``link"$, it returns the linear combination of independent variables, i.e., $\hat{\beta}_0+\hat{\beta}_1x_1+\ldots +\hat{\beta}_k x_k$. Alternatively, when $type = ``response"$, it returns the predicted probability of $y=1$. We can verify it by coding manually.
```{r}
### prediction 
new.ob = data.frame(nwifeinc = 10.91, educ = 12, exper = 14, expersq = 14^2,
                     age = 32, kidslt6 = 1, kidsge6 = 0)
predict(inlf.logit, newdata = new.ob, type = "link")
predict(inlf.logit, newdata = new.ob, type = "response")

# manual verification
xb <- sum(cbind(1, new.ob) * inlf.logit$coefficients); xb
phat <- exp(xb)/(1 + exp(xb)); phat
```

Lastly, let us discuss the interpretation of model. In the logit model, an estimated coefficient no longer measures the partial effect of the corresponding variable, as the partial effect of any independent variable now depends on values of all other independent variables (slide 9). In particular, for logit model, we can show that $g(z) = G(z)\cdot [1-G(z)]$. Consequently, we can calculate average partial effect for $x_j$ in a logit model using the formula $$n^{-1}\sum_{i=1}^n G(\hat{\beta}_0+\hat{\beta}_1x_{i1}+\cdots+\hat{\beta}_k x_{ik})\cdot [1-G(\hat{\beta}_0+\hat{\beta}_1x_{i1}+\cdots+\hat{\beta}_k x_{ik})]\hat{\beta}_j,$$ where $G(z)=\frac{\exp(z)}{1+\exp(z)}$. The code to calcuate the average partial effect is given below.
```{r}
# partial effects in logit model
mean(inlf.logit$fitted.values * (1 - inlf.logit$fitted.values)) * inlf.logit$coefficients
```
If we compare them against estimated coefficients from the linear probability model (Table 1), they are quite similar. This is common in practice - coeffients from the linear probablity model usually give reliable estimates of average partial effects of independent variables in the model.

One key difference between LPM and logit/probit is that the marginal effect is assumed to be constant with LPM, however, it is no longer the case for logit/probit. That is, with LPM, an extra kid always decreases the chance of labor force participation by 0.26, regardless how many kids the individual already has. However, intuitively, we would think that the first kid has the greatest effect, and the marginal impact of an extra kid shall decrease. This nonlinearity is reflected in logit/probit output. We can calculate the impact of having the first kid less than 6, and the second kid less than 6, using the code below. It turns out the first kid reduces the probability of being in the labor force by 0.35, while the effect of the second kid is around 0.22. This nonlinearity allows logit/probit fit the data better, as reflected by log-likelihoods shown in Table 1.
```{r}
# non-linear partial effects
new.ob <- with(data, data.frame(nwifeinc = mean(nwifeinc), educ = mean(educ), exper = mean(exper),
                    expersq = mean(expersq), age = mean(age), kidslt6 = c(0, 1, 2), kidsge6 = 1))

# partial effect of having the first kid less than 6
predict(inlf.logit, newdata = new.ob[2, ], type = "response") - 
        predict(inlf.logit, newdata = new.ob[1, ], type = "response")
# partial effect of having the second kid less than 6
predict(inlf.logit, newdata = new.ob[3, ], type = "response") - 
        predict(inlf.logit, newdata = new.ob[2, ], type = "response")
```