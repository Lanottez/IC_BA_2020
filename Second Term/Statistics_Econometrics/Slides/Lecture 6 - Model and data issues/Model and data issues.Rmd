---
title: "Model and Data Issues"
author: Jiahua Wu
subtitle: Statistics and Econometrics
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(car)
library(dplyr)
library(stargazer)
library(lmtest)
library(leaps)
```

# Part I
## Functional form misspecification
The R function for RESET (on slide 12) is $resettest()$, which is readily available from the $lmtest$ package. It is based on an F test for the expanded model in Step 2 with null hypothesis $H_0:\delta_{\hat{y}^2}=0,\delta_{\hat{y}^3}=0$. If we reject null, we reject the null hypothesis that functional form is correctly specified. Smaller the $p$-value, stronger the evidence against null. 
```{r}
load("hprice1.RData")

# RESET test
house.m1 <- lm(price ~ lotsize + sqrft + bdrms, data)
resettest(house.m1, type = "fitted")
```

Judging by the $p$-value, there is evidence of functional form misspecification in the model. As we mentioned in the class, one problem with RESET test is that it does not provide any direction on how to adjust the model when the null hypothesis is rejected. We will need to rely on theory/common sense/subject knowledge to guide the model construction.


## Variables Selection
In the class, we discussed two functions that will automatically evaluate different models, and return the best ones depending on goodness-of-fit measure of choice. The first function is $regsubsets()$ from $leaps$ package. It is based on an exhaustive search, and thus computational complexity increases exponentionally (in the number of independent variables).
```{r}
load("bwght.RData")
data.new <- na.omit(data)
bwght.model.search <- regsubsets(bwght ~ faminc + fatheduc + motheduc + parity 
                                 + male + white + cigs, data.new, nbest = 3)
```

Before running $regsubsets()$, we first remove any observations with missing values in the sample, such that different models are estimated with the same sample. $nbest$ specifies the number of subsets of each size to record. We plot the models based on two goodness-of-fit measures, namely $\bar{R}^2$ and $BIC$.
```{r}
plot(bwght.model.search, scale = "adjr2")
plot(bwght.model.search, scale = "bic")
```

The way to read the figures is as follows: each row indicates one model and each column indicates one independent variable. If a block is black (or grey, they are the same), it indicates that the corresponding independent variable (column) is included in the corresponding model (row). One thing worth-mentioning is that the vertical axis in the BIC figure indicates the difference of BICs between that particular model and a model with only the intercept, rather than the model BIC itself.

The other function we discussed for automatic variable selection is $step()$. It would perform stepwise search to find the model with the best goodness-of-fit measure. The function can search $forward$, $backward$, or in $both$ directions. With $forward$ search, at each iteration, the function would evaluate the model AICs by adding independent variables that are not already included in the model one at a time, and add the one with the lowest AIC at the end of the iteration. The algorithm stops when no more independent variables can be added to the model for a lower AIC. With $backward$ search, at each iteration, the function would evaluate the model AICs by removing independent variables that are already in the model one at a time, and remove the one such that the resulting model has the lowest AIC at the end of the iteration. 

In the end, $step()$ will return one model with the lowest AIC. As discussed in the class, we may find several models with similar AICs (with difference less than 2), then we can decide whether to include those variables (in one model but not the other) based on subject knowledge, and etc. There is no fixed rule here. Model validation (which you will learn in the machine learning class) could also help you to pick the model.
```{r, eval = FALSE}
# stepwise search
bwght.null <- lm(bwght ~ 1, data.new)
bwght.full <- lm(bwght ~ faminc + fatheduc + motheduc + parity + male + white 
                 + cigs, data.new)
step(bwght.null, scope = list(lower = bwght.null, upper = bwght.full), 
     direction = "forward")
step(bwght.full, direction = "backward")
step(bwght.null, scope = list(lower = bwght.null, upper = bwght.full), 
     direction = "both")
```

If we want to evaluate models based on BIC, we can simply add the argument $k=log(n)$ in the $step()$ function. 
```{r, eval = FALSE}
n <- nrow(data.new)
step(bwght.null, scope = list(lower = bwght.null, upper = bwght.full), 
     direction = "forward", k = log(n))
```


## Prediction
The function for model prediction in R is $predict()$. One argument that is worth mentioning is $interval$. If we specify $interval = ``confidence"$, then we are constructing a confidence interval for the average person - error term $u$ does not play a role here as on average it is equal to 0. On the other hand, if we specify $interval = ``predict"$, we are constructing an interval prediction for a particular individual. This is often called ``prediction interval". For prediction interval, we need to account for two sources of variations: sampling variation (as we don't really know the true population parameter), and variance in the error term (as we don't observe it for this particular individual). As such, the prediction interval would be much larger.
```{r}
load("wage1.RData")
wage.m1 <- lm(wage ~ educ, data)
newdata <- data.frame(educ = 12)

# prediction for an average person
predict(wage.m1, newdata, interval = "confidence", level = 0.95)

# prediction for a particular individual
predict(wage.m1, newdata, interval = "predict", level = 0.95)
```

Lastly, if we want to predict $y$ in a model where the dependent variable is $log(y)$, simply exponentiating the fitted value wouldn't give us the right answer. We need to scale it up by a sample estimate of $E(exp(u))$, which can be estimated by calculating the sample average of exponential of residuals.
```{r}
wage.m2 <- lm(log(wage) ~ educ, data)
predicted.logwage <- predict(wage.m2, newdata, interval = "none")
predicted.wage <- mean(exp(wage.m2$residuals)) * exp(predicted.logwage)
```


# Part II
## Outlier and leverage points
One common way to identify outliers/leverage points in the sample is through visual inspection. Plots that involve standardized residuals, leverage values or Cook's distances can all help us with this regard. We draw two plots for this particular example. The first one is "standardized residuals vs fitted values", which can help us identify outliers (unusual $y$ values). The rule of thumb is an observation is considered as an outlier if it lies 3 standard deviations away from the mean. Judging from the plot, we have one outlier in this example, just above 3 standard deviations from the mean.

The second plot is "Cook's distance vs leverage values", which can help us identify high leverage points (unusual $x$ values), and influential observations. An observation has high leverage if its leverage value is greater than twice of the mean ($2(k+1)/n$). Influential observations are those ones with Cook's distance greater than 1.
```{r}
load("rdchem.RData")
rdchem.m1 <- lm(rdintens ~ sales + profmarg, data)

ggplot(rdchem.m1, aes(.fitted, .stdresid)) + geom_point() + stat_smooth(method = "loess") + 
  xlab("Fitted Value") + ylab("Standardized Residuals")
ggplot(rdchem.m1, aes(.hat, .cooksd)) + geom_point() + stat_smooth(method = "loess") + 
  xlab("Leverage") + ylab("Cook's Distance")
```

In the example, we have one influential observation with Cook's distance around 3.13, which implies that our OLS regression function would change significantly with or without this observation.
```{r}
# standardized residual
rstandard(rdchem.m1)

# leverage value
hatvalues(rdchem.m1)

# cooks distance
cooks.distance(rdchem.m1)
```
Judging by the Cook's distance, the influential observation is observation 10. So if we drop this observation and run regression again, firm's size now has a significant impact on R&D expenditures.
```{r}
data.new <- data[-c(10), ]
rdchem.m2 <- lm(rdintens ~ sales + profmarg, data.new)
summary(rdchem.m2)
```