{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "\n",
    "_Data Structures and Algorithms_\n",
    "\n",
    "_Imperial College Business School_\n",
    "\n",
    "\n",
    "---\n",
    "In a previous notebook, we worked with the `numpy` library, which is the standard library for numeric manipulations in Python.\n",
    "\n",
    "In this notebook, we will move to `pandas`, a library that builds on top of numpy with higher level structures to allow convenient and versatile data analysis. Here we will cover the use of pandas for loading data, representing it with common structures (pandas series and dataframes), and manipulating/accessing data within these structures. \n",
    "\n",
    "In the extra part of the notebook, we will briefly look at more complicated data operations such as merging data, and introduce the `scikit-learn` library for machine learning in Python.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting your work\n",
    "\n",
    "There are five assessed exercises in this notebook. You will get full credit for making a good attempt at most of the questions.\n",
    "\n",
    "After you're done with the exercises below, you can submit the assignment to OK directly from this Notebook. The submission instructions are in the end of the main part of the notebook.\n",
    "\n",
    "First, let's connect the Notebook to OK. To do so, run the code cell below. It may prompt you to log in. If it does, follow the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Don't change this cell; just run it. \n",
    "# The result will give you directions on how to log in to the OK submission system.\n",
    "# Once you're logged in, OK should remember it for the duration of the session.\n",
    "import zipimport\n",
    "import os\n",
    "nb_path = os.path.join('client', 'api', 'notebook')\n",
    "ok_bundle =  zipimport.zipimporter('./ok').load_module('client')\n",
    "ok_nb = zipimport.zipimporter('./ok').load_module(nb_path)\n",
    "ok = ok_nb.Notebook('ses10.ok')\n",
    "_ = ok.auth(inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell provides an alternative way to log in to OK if the above way fails on your computer for some reason. If the login worked, you can skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# If your login works with the above cell, no need to run this\n",
    "# If the above cell does not work, this is a backup way of loading up OK\n",
    "# Don't change this cell; just run it. \n",
    "!pip install -U okpy\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('ses10.ok')\n",
    "_ = ok.auth(inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the OK problem still persists, try the following steps: \n",
    "1. Go to the command line and navigate to the folder where you have this Notebook.\n",
    "2. Run the command `python ok` and log in the standard way\n",
    "3. Try running the first cell above again.\n",
    "The questions ask you to use Python. You can do so below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Pandas builds on top of numpy and matplotlib to provide data analysis and statistical functionalities in Python. \n",
    "\n",
    "We will first introduce some core aspects of pandas using toy data, and then analyse a real data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # We import the numpy library to generate some random data.\n",
    "# # Also the pandas library is imported.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# # We fix the seed so that the results are reproducible.\n",
    "# # Please do not change the seed below.\n",
    "np.random.seed(seed=9)\n",
    "# # Let's generate some toy data (sampled from a Gaussian distribution)\n",
    "values = np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the work we do with pandas revolves around the use of _DataFrames_ to organize data. A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet. If you've used R, chances are you've already used dataframes. The functionalities are very similar. The data structures in pandas build on and wrap around most popular data-types of python, e.g. lists, numpy arrays, so it's easy to move from one to another and call functions that work on a specific structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Let's use the numpy vector that we created above to create a DataFrame.\n",
    "dataframe = pd.DataFrame(data=values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pandas also includes most of the core methods/properties of a numpy array.\n",
    "# # For instance, you can call get the shape the same way as in a numpy array.\n",
    "print(dataframe.shape)\n",
    "\n",
    "# # Few additional methods that are exactly the same (in the way you call them)\n",
    "# # as in the numpy arrays are the max, min, median, mean, sum, std (standard \n",
    "# # deviation) methods.\n",
    "print(dataframe.max())\n",
    "\n",
    "# # The max value should be 2.45. But notice the result is not a simple value \n",
    "# # but a pandas data \"series\"\n",
    "print('Result type:', type(dataframe.max()))\n",
    "\n",
    "# # To get only the max value without the additional information, you \n",
    "# # could do: \n",
    "print(dataframe.max()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Dataframe summary\n",
    "\n",
    "**A.** What is the min value of the dataframe?\n",
    "\n",
    "**B.** What is the mean value of the dataframe?\n",
    "\n",
    "**C.** Now create a new DataFrame named `dataframe_10k` that includes a vector of 10000 elements sampled from a Gaussian as above. What is the mean value now? **Hint** Remember to copy the same seed as above to ensure that you create the same array as in the test. The random.seed and the creation of the matrix should be in the same cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Change the next line so that min_elem_df contains the min\n",
    "# element of dataframe. \n",
    "# The expected result is ONLY the min value.\n",
    "min_elem_df = ...\n",
    "\n",
    "# Change the next line so that mean_elem_df contains the mean\n",
    "# element of dataframe. \n",
    "# The expected result is ONLY the mean value.\n",
    "mean_elem_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Please do not modify the line below (allows us to replicate the results).\n",
    "np.random.seed(seed=9)\n",
    "\n",
    "# The lines below concern part C of Question 1. \n",
    "# Modify the line below so that it creates the dataframe of 10k elements\n",
    "# as mentioned in the description (part C above).\n",
    "dataframe_10k = ...\n",
    "\n",
    "# Change the next line so that mean_10k contains the mean\n",
    "# element of dataframe_10k. \n",
    "mean_10k = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test cell; please do not change!\n",
    "_ = ok.grade('q1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how a DataFrame is initiated and some methods that it includes, you might wonder the benefits over using a numpy array. \n",
    "\n",
    "Here are some important differences between the two:\n",
    "\n",
    "* A numpy array demands homogeneous data, while in a dataframe, different data types (float, string, datetime) are allowed in the same structure.\n",
    "* Numpy is an amazing low-level tool for data manipulation, required by most other libraries (including pandas). However, pandas offers a  plethora of high-level functionality, e.g. grouping data by conditions (using the groupby method) and combining datasets with merge and join methods. We will see these methods later.\n",
    "* The data (columns/rows) can have labels in pandas. In numpy the programmer needs to keep in mind the semantics of each column/row, while in dataframe they are explicitly coded in the structure.\n",
    "\n",
    "Let's study an example that utilises some of these ideas. We'll create a small dataset of a few countries with the countries names, population, GDP, a d country codes. This sounds like a problem where we might use dictionary. Indeed the dictionary is convenient for housing this kind of data. However, data/number processing with a dictionary can be cumbersome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "d1 = {}\n",
    "d1['countries'] = ['UK', 'France', 'Spain', 'Netherlands']\n",
    "d1['codes'] = ['uk', 'fr', 'es', 'nl']\n",
    "# # the population is measured in millions\n",
    "d1['population'] = [65.6, 66.9, 46.6, 17.0]\n",
    "# # the gdp is measured in billions\n",
    "d1['gdp'] = [2619, 2465, 1232, 770]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's create a dataframe now with that data.\n",
    "# # DataFrame includes a convenience constructor that\n",
    "# # just accepts the dictionary data and creates\n",
    "# # the same structure as in the previous example.\n",
    "countries_data = pd.DataFrame(d1)\n",
    "\n",
    "print(countries_data['gdp'])\n",
    "countries_data # Notebook gives a nice HTML table of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additionally, we can call the aggregation methods as above, but now we get\n",
    "# # a result per column (which makes sense, we do not want to average\n",
    "# # gdps together with populations).\n",
    "countries_data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: More summaries\n",
    "\n",
    "**A.** What is the sum of the populations in the `countries_data`?\n",
    "\n",
    "**B.** What is the standard deviation of the gdp's in the `countries_data`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Change the next line so that sum_pop_countries_data computes the sum of\n",
    "# the populations from countries_data, rounded to one decimal.\n",
    "sum_pop_countries_data = ...\n",
    "\n",
    "# Change the next line so that std_gdp_countries_data computes the standard deviation of\n",
    "# the gdp from countries_data, rounded to one decimal.\n",
    "std_gdp_countries_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test cell; please do not change!\n",
    "_ = ok.grade('q2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File loading, data processing with Pandas\n",
    "\n",
    "Now that you've become familiar with Dataframe, let's scale up our exploration and download a real dataset. We will use the Titanic dataset from the Kaggle Getting Started challenge at:\n",
    "\n",
    "https://www.kaggle.com/c/titanic-gettingStarted\n",
    "\n",
    "The dataset is included as `titanic.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# make sure matplotlib plots display nicely in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's load the first csv file. \n",
    "data = pd.read_csv('titanic.csv')\n",
    "\n",
    "# # Printing the shape of the dataset we have just loaded.\n",
    "print(data.shape)\n",
    "\n",
    "# # The first step in data analysis is the exploration step.\n",
    "# # We want to verify that a) our dataset is appropriately loaded,\n",
    "# # b) get a sense of what values it has.\n",
    "# # Let's display the 5 first rows:\n",
    "data.head(5)\n",
    "# # (When we run this in the Notebook, we will get a nice \n",
    "# #  HTML representation of the table.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract some info from what we have just printed:\n",
    "\n",
    "As you've noticed, each row has an id, starting from zero, and the data columns have names that help us categorise the values of the columns. For instance, the fourth column includes the names of the passengers and the sixth their ages. \n",
    "In the Cabin column, notice that there are some 'NaN' values. 'NaN' typically denotes a missing value in pandas.\n",
    "\n",
    "Pandas includes a lot of built-in tools and methods that produce useful insights for our data. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pandas also allow us to plot values directly. \n",
    "# # Let's plot a histogram of the age of the passengers\n",
    "# # pandas wraps around the relevant matplotlib function to directly do plotting. \n",
    "# # We could also import matplotlib as before\n",
    "data.hist(column='Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For accessing a specific value of a column you can you use the at[] property or the get() method, like this:\n",
    "\n",
    "```python\n",
    ">>> data.at[0, 'Age']\n",
    "22\n",
    ">>> data.get('Age')\n",
    "# the entire column\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Accessing a dataframe\n",
    "\n",
    "**A.** What is the age of the 10th passenger (i.e. PassengerId is 10)?\n",
    "\n",
    "**B.** What is the cabin value for the 194th passenger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Change the next line so that age_passenger_10 computes the age of\n",
    "# the 10th passenger.\n",
    "age_passenger_10 = ...\n",
    "\n",
    "# Change the next line so that cabin_194_passenger computes the \n",
    "# cabin number of the 194th passenger.\n",
    "cabin_194_passenger = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test cell; please do not change!\n",
    "_ = ok.grade('q3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: More accessing\n",
    "\n",
    "Fill in the line of code below and then test it using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the next line so that it computes ticket number/id of\n",
    "# the 100th passenger\n",
    "ticket_i_th_passenger = ...\n",
    "\n",
    "# We've put this line in this cell so that it will print\n",
    "# the value you've given to ticket_i_th_passenger when you\n",
    "# run it.  You don't need to change this.\n",
    "ticket_i_th_passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test cell; please do not change!\n",
    "_ = ok.grade('q4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data modification with pandas\n",
    "\n",
    "In addition to parsing data, pandas can be used to modify data tables. We will go through a few common methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's delete the first and the third \n",
    "# # passengers (remember the indexing in \n",
    "# # python starts from 0).\n",
    "data.drop([0, 2], axis=0).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apart from that, you can also delete whole columns or rows.\n",
    "# # For instance, for your problem, the Cabin column might be \n",
    "# # irrelevant, let's delete it.\n",
    "data.drop(['Cabin'], axis=1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print the first five elements to check how the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a minute to consider what pandas have printed for you...\n",
    "\n",
    "You might have observed, that the 'Cabin' column that we deleted above (the result showed it was deleted above) is still there. You will also notice, that even the elements that we deleted (passengers 1 and 3) are also re-added. Or were they never deleted in the first place? \n",
    "\n",
    "By default in pandas `drop` is not \"inplace\": in other words, the function returns you a *copy* while the original is untouched. However, since the copy is of the same type, you can assign it to a new variable, which will now contain only the reduced elements/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's check that the data type is the same.\n",
    "print(type(data))\n",
    "# # What about the return type from a drop operation?\n",
    "print(type(data.drop(['Cabin'], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Assign to the variable `reduced_data` the pandas matrix that does not include the columns of Cabin, Embarked and SibSp. Then execute the cell below for testing with the ok system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Change the next line so that it computes the reduced data matrix.\n",
    "reduced_data = ...\n",
    "\n",
    "# We've put this line in this cell so that it will print\n",
    "# the value you've given to reduced_data when you\n",
    "# run it.  You don't need to change this.\n",
    "reduced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell; please do not change!\n",
    "_ = ok.grade('q5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from deleting, you can also replace values by new ones. Remember that the main purpose of pandas is statistical computation, hence we often translate values to numbers that we know how to process. \n",
    "\n",
    "For instance, strings, such as 'male' or 'female' are not really useful for statistical analysis. We usually prefere to replace them with numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('male', 1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with drop, the replace function returns a copy, so keep in mind that if you want to save them, you have to assign to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # An alternative way to replace the data is the following:\n",
    "data['Sex'] = data['Sex'].map({'female': 1, 'male': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now assume that from an external source, you figure out the Nationality of the passengers and want to insert that information. Pandas allow you to insert new rows, and in contrast with the aforementioned methods, this is an in-place operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Right after the Sex, we want to include a new field named 'Nationality'. \n",
    "# # Since most of the passengers are Irish, we will by default assign \n",
    "# # the label 'Irish' to them and refine for those that are not.\n",
    "data.insert(5, 'Nationality', 'Irish')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have replaced the values of a complete column, however what if we want to perform some modifications in specific values per row (e.g. if a condition is true)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Let's assume for a moment that the nationality of \n",
    "# # those with Age NaN is Other European (hence why there \n",
    "# # are no records of their age).\n",
    "# # We want to replace the default nationality with\n",
    "# # their known nationality.\n",
    "import numpy as np\n",
    "for index, row in data.iterrows():\n",
    "    if np.isnan(data.loc[index, \"Age\"]):\n",
    "        data.loc[index, \"Nationality\"] = \"European\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What we've done above is to replace some of the nationalities with 'European'. \n",
    "# # One property of pandas that we've utilised for that is the '.loc'.\n",
    "# # Let's explore that a bit more: \n",
    "print(data.loc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be easily verified from the print above, this provides the whole row of the 4th passenger (as Python follows zero-based indexing). In other words, data.loc is a way to index a row or even a specific 'cell' inside the row as we did above with `data.loc[index, \"Nationality\"]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # As typically done in higher level libraries in python, '.loc' offers a great deal of functionality. \n",
    "# # You can find furthr information by executing data.loc??\n",
    "help(data.loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter dataframes directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_bool = data['Nationality'] == 'European' # creates Booleans for each entry\n",
    "print(european_bool.head(10))\n",
    "# We could then pick only these records with \n",
    "europeans = data[european_bool]\n",
    "# If we want to count different values, we can do \n",
    "data['Nationality'].value_counts()\n",
    "# Counting how many non null values exist would be data['Nationality'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "**Important**. Run this final cell to submit your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional exercises\n",
    "\n",
    "### Question 6\n",
    "\n",
    "**A.** Can you replace the nationalit column with numbers? For instance, try to assign values such that Irish = 0, Other European = 1. \n",
    "\n",
    "**B.** How many people of European nationality are there? (Following the assumption above for the NaN in the age)\n",
    "\n",
    "**C.** For how many passengers do we have with Cabin information?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Change the next line so that it computes the number of\n",
    "# people with European nationality.\n",
    "n_european = ...\n",
    "\n",
    "# Change the next line so that it computes the number of\n",
    "# people for which we have cabin information.\n",
    "n_passengers_cabin = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test cell; please do not change!\n",
    "_ = ok.grade('q6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Assign to the variable `only_pclass2` the pandas matrix that includes only the passengers with Pclass = 2. Then execute the cell below for testing with the ok system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Change the next line so that it computes a new dataframe\n",
    "# that includes only the people with Pclass = 2.\n",
    "# One way to use conditions in pandas is directly with data[CONDITION]\n",
    "only_pclass2 = ...\n",
    "\n",
    "# We've put this line in this cell so that it will print\n",
    "# the value you've given to only_pclass2 when you\n",
    "# run it.  You don't need to change this.\n",
    "only_pclass2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test cell; please do not change!\n",
    "_ = ok.grade('q7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra part: More Titanic\n",
    "\n",
    "The session folder contains the files `titanic.csv` and `nationalities.csv`. The latter file contains nationalities of Titanic passengers. Note that this file was created solely for the purpose of learning within this course, and should not be used outside of the scope of the course. It does **not** reflect the real nationalities of the passengers.\n",
    "\n",
    "Load the files titanic.csv and nationalities.csv in the variables `data_org` and `data_nat` respectively. Use the cell below as instructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reads the file titanic.csv\n",
    "data_org = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Reads the file nationalities.csv\n",
    "data_nat = pd.read_csv('nationalities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced pandas dataframe processing\n",
    "\n",
    "When working on data analysis, we often need to combine information from different sources, or produce our own data and then combine them with other sources. The pandas library offers a great deal of methods to facilitate this process. We will study below the functionality of merging datasets. We will merge the titanic.csv (original file) and the nationalities.csv (data produced by our research). \n",
    "\n",
    "Before merging two datasets, we need to know exactly how these datasets are related, how they are structured and whether they already have some common fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # First, let's ensure that the two datasets have the same number of elements (passenger data).\n",
    "assert data_org.shape[0] == data_nat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's print the heads of the two datasets to figure out if there are any common elements.\n",
    "data_org.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We are ready to perform the merging. Observe that the datasets share a common column in\n",
    "# # the PassengerId, so we will use it to combine the data.\n",
    "data_new = data_org.merge(data_nat, on='PassengerId')\n",
    "\n",
    "# # Let's see what we've created now.\n",
    "data_new.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pandas offers several convenient methods for conditional selection and actions on them.\n",
    "# # For instance, above we worked on getting useful aggregate statistics\n",
    "# # over the whole dataset (do you remember the commands?).\n",
    "# # However, often we'd like to select only a subset of the data based on some condition. \n",
    "# # For instance, let's say we would like to print the average age per class.\n",
    "# # One way to do that would be to iterate over all the elements, create a list, sum them \n",
    "# # and then compute the average. \n",
    "# # But pandas conveniently allows us to do it with a single command. \n",
    "# # It works as follows: \n",
    "# # First we group the data by the class, then we ask pandas to compute the mean of the age.\n",
    "print(data_new.groupby(['Pclass'])['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In the command above, we've averaged both men and women based only on the Pclass.\n",
    "# # However, we could separate the two sexes and compute the mean for each sex.\n",
    "print(data_new.groupby(['Pclass', 'Sex'])['Age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noted that in class three, the average age of each sex differs significantly with the mean being closer to the male average age. Intuitively, you expect to find more men in that class than women. However, what is the command to find the exact number of males in this class? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # We will now drop few columns that contain strings to mention few methods for statistical processing.\n",
    "data = data_new.drop(['Cabin', 'Name', 'Ticket', 'Embarked', 'Nationality', 'Sex'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # As we've seen, pandas bundles much of the functionality of numpy and lists.\n",
    "# # We can for example \"clip\" the values, i.e. restrict them in a chosen interval.\n",
    "# # Notice that some values were greater than our upper bound, but are now\n",
    "# # restricted to the maximum upper bound we set.\n",
    "data.clip(lower=0, upper=40).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If the method we would like to apply to the data does not exist, we can use the \n",
    "# # '.apply' method that allows us to choose any function to be applied to each record.\n",
    "# # One way to do this is through an \"anonymous\" lambda function.\n",
    "# # This works as defining a function without an explicit name to apply to each record\n",
    "import numpy as np\n",
    "data[\"SurvivedPlusOne\"] = data[\"Survived\"].apply(lambda x: x + 1)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Who survived? Use the aggregation functions above to calculate survival probabilities based on fare classes, age, or fare paid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra topic: Introduction to Machine Learning\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "We can think of Machine Learning (ML) as a way to simulate a function. Not necessarily a function with the strict mathematical sense of calculus courses, but rather a function that maps some (optional) input to some output. \n",
    "\n",
    "For instance, translating English to German can be thought as a function that maps text in the English language to the respective text into German. Let's think about that for a minute: The naive approach is to translate word by word, e.g. having a python dictionary with keywords the english words and the respective value to be a german word. However, this approach would lose most of the language's syntax, the connections and the higher level representations of the text. Machine-learning based translation tools seek to bridge this cap and are a very active field in Machine Learning research. \n",
    "\n",
    "Machine Learning approaches typically use some **tunable parameters** (typically an array of floating point values) that are adjusted (learnt) so as to improve their behaviour by **adapting to previously seen data.**\n",
    "\n",
    "Depending on the ML application, you can have different dimensions of data. For example, in image processing, they typically deal with 2D arrays of shape  ``[n_samples x n_features]``. The number of features is the same for each object, and each feature column refers to a related piece of information about each sample. In financial analysis, you also deal with 2D arrays where n_features would be the number of observations in a time series, e.g. the stock price of a company for the past several years. The different samples would then refer to different companies. \n",
    "\n",
    "Machine-learning methods are often divided into *supervised learning* and *unsupervised learning*.\n",
    "\n",
    "Supervised learning relies on having some labelled sample data as input, and training a model to analyse new instances of related data. For example, we might have pictures of different individuals and label each picture with the person who's in the photo, and then train an algorithm to classify new pictures. \n",
    "\n",
    "Unsupervised learning, in contrast, does not require any labelled samples to learn. With the photos, we might instead use unsupervised learning to try to divide our pictures into groups of different people without any upfront labelling. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning: Classification and regression\n",
    "\n",
    "In **Supervised Learning**, we have a dataset consisting of different **features and labels**.\n",
    "The task is to construct an estimator which is able to predict the label of an object\n",
    "given the set of features. Some examples are:\n",
    "\n",
    "- given a multicolor image of an object through a telescope, determine\n",
    "  whether that object is a star, a quasar, or a galaxy.\n",
    "- given a photograph of a person, identify the person in the photo.\n",
    "- given a list of movies a person has watched and their personal rating\n",
    "  of the movie, recommend a list of movies they would like\n",
    "  (So-called *recommender systems*: a famous example is the [Netflix Prize](http://en.wikipedia.org/wiki/Netflix_prize)).\n",
    "- given observations from a site (e.g. sensors' input), figure out\n",
    "  whether the facility works as predicted or there are \n",
    "  some disruptions (this task is alleged \"anomaly detection\").\n",
    "\n",
    "What these tasks have in common is that there is one or more unknown\n",
    "quantities associated with the object which needs to be determined from other\n",
    "observed quantities.\n",
    "\n",
    "Supervised learning is further broken down into two categories, **classification** and **regression**.\n",
    "In classification, the label is discrete, while in regression, the label is continuous. For example,\n",
    "in astronomy, the task of determining whether an object is a star, a galaxy, or a quasar is a\n",
    "classification problem: the label is from three distinct categories. On the other hand, we might\n",
    "wish to estimate the age of an object based on such observations: this would be a regression problem,\n",
    "because the label (age) is a continuous quantity.\n",
    "\n",
    "---\n",
    "\n",
    "Both regression and classification are some tasks essential in several fields that require data analysis (e.g. financial analysis). Some examples are:\n",
    "- given historical observations of the house market prices, predict the future values.\n",
    "- given the offers (for outsourcing product $y$ creation) from $v$ different companies, decide with which company to collaborate. \n",
    "- given a history of stock market prices, predict whether there is a disruptive event at some time $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no tests for these exercises.\n",
    "\n",
    "**Exercise.** What type of problem (classification/regression) is the one with house market prices?\n",
    "\n",
    "**Exercise.** What about the offers for outsourcing the product creation?\n",
    "\n",
    "**Exercise.** What type of problem is the prediction of disruptive events in stock market?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit learn\n",
    "We'll use a popular machine-learning library called **Scikit learn**. \n",
    "You can start using scikit straight away, as the API is intuitive and well documented. They also offer a plethora of more advanced options if you want to customise the parameters of the optimisation problem.\n",
    "\n",
    "In this tutorial, we focus on regression.  We'll begin with the most standard regression setting: the linear regression one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # We import only the LinearRegression class.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "# # Import the modules for printing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you want to check out the documentation, \n",
    "# # you can call help().\n",
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Let's initialise a new Linear Regression model.\n",
    "model = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In most python libraries, you can print the objects and\n",
    "# # acquire information about their class/parameters.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elaborate on 'Parameters', please...\n",
    "At this point, we should clarify the use of the term **parameters**. \n",
    "\n",
    "In the `help` command a few cells above, you might have noticed that under the title 'Parameters' are the arguments of the class. Those are the programming parameters, provided by the programmer. Those are often called **hyper-parameters**. \n",
    "\n",
    "However, when we mentioned above that ML applications include parameters that are learnt, we did not mean the hyper-parameters that are decided (semi-)automatically. What we meant with **tunable parameters** were the model's parameters. Every model requires different parameters, e.g. a simple linear regression of 1D data has the following two parameters: the slope and the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # starting from some toy data to get familiar with the concepts:\n",
    "X = np.array([[0],\n",
    "              [1],\n",
    "              [2]]) # A 2D array with 3 samples of 1 feature each\n",
    "y = np.array([[0, 1, 2 ]]).T # What we want to predict as a function of X\n",
    "# # Hint: This is the identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's plot to verify that we've defined the identity function.\n",
    "_ = plt.plot(X[:, 0], y, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's fit the Linear regression model we've defined above.\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It was that simple. We call the method fit, provide \n",
    "# # the training samples and the model \"learns\" the parameters.\n",
    "# # Let's now print the coefficient learnt for this case:\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, it's time to move to a more useful example. We will be given some samples X (1D) and try to estimate the linear line that created them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # The seed below is significant for reproducible results.\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.random.random(size=(20, 1))\n",
    "y = 3 * X[:, 0] + 2 + np.random.normal(size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit linear regression to it.\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "m1 = 'Model coefficient: {}, and intercept: {}'\n",
    "print(m1.format(model.coef_, model.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Let's now generate some new samples (UNSEEN in the training)\n",
    "# # and use those to predict the y values.\n",
    "X_test = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot with blue dots the 'training' points, while with a red line the\n",
    "# estimated line.\n",
    "plt.plot(X, y, 'bo')\n",
    "plt.plot(X_test, y_test, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you increase the number of training points from 100 to 10K?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A recap on Scikit-learn's estimator interface\n",
    "\n",
    "As you have observed, scikit-learn provides a uniform interface across all methods.In the case of regression, the main functions you will need are:\n",
    "\n",
    "+ `model.fit()` : fit training data. For supervised learning applications,\n",
    "this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
    "For unsupervised learning applications, this accepts only a single argument,\n",
    "the data `X` (e.g. `model.fit(X)`).\n",
    "+ `model.predict()` : given a trained model, predict the label of a new set of data.\n",
    "This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
    "and returns the learned label for each object in the array.\n",
    "+ `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
    "a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now seen how to fit a standard linear regression model with scikit-learn. For statistical analysis, such as hypothesis testing, we would often use another library called [statsmodels](http://www.statsmodels.org/stable/index.html).\n",
    "\n",
    "Now, let's work through a machine-learning application with the Titanic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning task: Predict survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ the Titanic data to predict the chance of survival of the passengers. In other words, the goal of the this part is to predict whether a passenger survived based on other known attributes. This is a very brief introduction and we will not go through all details of the models. When you see a model or a command you're not familiar with, it's a good idea to try to search online for what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_column = data['Survived']\n",
    "survived_column.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the value is 1 if the person survived, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the groupby as we used it above to get the survivors per category:\n",
    "data.groupby('Survived').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(survived_column == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, ~38% of the people survived. Let's convert the values of survived into a numpy arrays (input to scikit-learn).\n",
    "\n",
    "`sklearn` estimators all work with homegeneous numerical feature descriptors passed as a numpy array. Therefore passing the raw data frame will not work out of the box.\n",
    "\n",
    "Let us start simple and build a first model that only uses readily available numerical features as input, namely `data.Fare`, `data.Pclass` and `data.Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = survived_column.values\n",
    "numerical_features = data[['Fare', 'Pclass', 'Age']]\n",
    "numerical_features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight issue is that passenger 6 has an age 'NaN', which will not be recognised from sklearn. In general, the first step of any machine learning application is to pre-process the data. \n",
    "\n",
    "Typically, the data are (un)normalised, in a different format than your library requires them, etc. So, you will need to take care that they include the right values at first, otherwise you might end up spending hours trying to understand why the trained models return non-sensical results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_features = numerical_features.dropna().median()\n",
    "median_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this value to replace the missing ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_features = numerical_features.fillna(median_features)\n",
    "imputed_features.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data frame is clean, we can convert it into an homogeneous numpy array of floating point values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "features_array = imputed_features.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 891 samples: let us keep 700 for training our model and the rest to test the quality of said model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train = features_array[:700, :]\n",
    "X_test = features_array[700:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's always a good idea to verify that your data have the expected shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_train = target[:700]\n",
    "y_test = target[700:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally apply a regression model! We'll use [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), common for modelling binary data such as survivals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1.)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "target_predicted = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72 percent accuracy, not **too** bad..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation and interpretation\n",
    "\n",
    "#### Interpreting linear model weights\n",
    "\n",
    "The `coef_` attribute of a fitted linear model such as `LogisticRegression` holds the weights of each features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = numerical_features.columns\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "plt.bar(x, logreg.coef_.ravel())\n",
    "_ = plt.xticks(x + 0.5, feature_names, rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, survival is slightly positively linked with Fare (the higher the fare, the higher the likelyhood the model will predict survival) while passenger from first class and lower ages are predicted to survive more often than older people from the 3rd class.\n",
    "\n",
    "First-class cabins were closer to the lifeboats and children and women reportedly had the priority. Our model seems to capture that historical information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's your first machine-learning algorithm. Even though you can build much more powerful models, standard linear (or logistic) regression is often remarkably effective for providing an understanding of the complexity of your data. That is, if this error is already quite small with the linear regression model, then you might as well use that.\n",
    "\n",
    "For a much more detailed introduction to machine learning in Python, see\n",
    "- The Python Data Science Handbook, Chapter 5, https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.00-Machine-Learning.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
